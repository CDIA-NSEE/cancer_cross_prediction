{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPrAw5M2GIElomW0OJWo+0I"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","from sklearn.impute import KNNImputer\n","\n","# Graphs\n","import matplotlib.pyplot as plt\n","\n","# Preprocessing\n","from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, MaxAbsScaler, QuantileTransformer, OneHotEncoder\n","from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n","from imblearn.over_sampling import SMOTE\n","\n","# Metrics\n","from sklearn.metrics import roc_curve, roc_auc_score, auc, ConfusionMatrixDisplay, classification_report, confusion_matrix\n","from sklearn import tree\n","\n","# SHAP values\n","import shap"],"metadata":{"id":"X-3C0xa5uXNH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def data_prep(df, topogrup):\n","    \"\"\"\n","    Prepares the dataset for analysis by applying filters and transformations.\n","\n","    Args:\n","        df (pandas.DataFrame): The input DataFrame containing the raw data.\n","        topogrup (list): A list of topography group codes to filter the data.\n","\n","    Returns:\n","        pandas.DataFrame: The prepared DataFrame ready for analysis.\n","\n","    \"\"\"\n","\n","    df_aux = df.copy()\n","\n","    # Filter data based on topography group\n","    df_aux = df_aux[df_aux.TOPOGRUP.isin(topogrup)]\n","\n","    # Filter for individuals aged 19 and above\n","    df_aux = df_aux[df_aux.IDADE > 19]\n","\n","    # Filter for residents of the state of SP\n","    df_aux = df_aux[df_aux.UFRESID == 'SP']\n","\n","    # Filter out specific ECGRUP values\n","    df_aux = df_aux[~df_aux.ECGRUP.isin(['0','X','Y'])]\n","\n","    # Filter for cases with microscopic confirmation\n","    df_aux = df_aux[df_aux.BASEDIAG == 3]\n","\n","    # Convert date columns to datetime objects\n","    list_datas = ['DTCONSULT', 'DTDIAG', 'DTTRAT', 'DTULTINFO']\n","    for col_data in list_datas:\n","        df_aux[col_data] = pd.to_datetime(df_aux[col_data])\n","\n","    # Calculate time differences in days\n","    df_aux['CONSDIAG'] = (df_aux.DTDIAG - df_aux.DTCONSULT).dt.days\n","    df_aux['DIAGTRAT'] = (df_aux.DTTRAT - df_aux.DTDIAG).dt.days\n","    df_aux['TRATCONS'] = (df_aux.DTTRAT - df_aux.DTCONSULT).dt.days\n","    df_aux['ULTIDIAG'] = (df_aux.DTULTINFO - df_aux.DTDIAG).dt.days\n","    df_aux[['DIAGTRAT', 'TRATCONS']] = df_aux[['DIAGTRAT', 'TRATCONS']].fillna(-1)\n","\n","    # Categorize time differences\n","    df_aux.CONSDIAG = [0 if consdiag <= 30 else 1 if consdiag <= 60 else 2 for consdiag in df_aux.CONSDIAG]\n","    df_aux.TRATCONS = [99 if tratcons < 0 else 0 if tratcons <= 60 else 1 if tratcons <= 90 else 2 for tratcons in df_aux.TRATCONS]\n","    df_aux.DIAGTRAT = [99 if diagtrat < 0 else 0 if diagtrat <= 60 else 1 if diagtrat <= 90 else 2 for diagtrat in df_aux.DIAGTRAT]\n","\n","    # Impute missing values in ESCOLARI using KNN\n","    cols_input = ['ESCOLARI', 'IDADE', 'ECGRUP', 'SEXO']\n","\n","    df_esc = df_aux[cols_input].copy()\n","    df_esc.loc[df_esc.ESCOLARI == 9, 'ESCOLARI'] = np.nan\n","    df_esc.loc[df_esc.ECGRUP == 'I', 'ECGRUP'] = 1\n","    df_esc.loc[df_esc.ECGRUP == 'II', 'ECGRUP'] = 2\n","    df_esc.loc[df_esc.ECGRUP == 'III', 'ECGRUP'] = 3\n","    df_esc.loc[df_esc.ECGRUP == 'IV', 'ECGRUP'] = 4\n","\n","    X = df_esc.values\n","    imputer = KNNImputer(n_neighbors=5, weights='distance')\n","    df_aux['ESCOLARI_preench'] = imputer.fit_transform(X)[:,0].round()\n","\n","    # Create a new feature indicating if treatment was in the same city\n","    df_aux['IBGE_idem_IBGEATEN'] = 0\n","    df_aux.loc[df_aux.IBGE == df_aux.IBGEATEN, 'IBGE_idem_IBGEATEN'] = 1\n","\n","    # Create a new column 'presenca_rec' to indicate the presence of recurrence\n","    # 0: No recurrence\n","    # 1: Recurrence\n","    df_aux['presenca_rec'] = [0 if rec == 1 else 1 for rec in df_aux.RECNENHUM]\n","\n","    # Correct for cases where RECNENHUM is 0 but DTRECIDIVA is not null\n","    df_aux.loc[(df_aux.presenca_rec == 0) & (df_aux.DTRECIDIVA.notnull()), 'presenca_rec'] = 1\n","\n","    # Extract only the number from the DRS column\n","    DRS_expand = df_aux.DRS.str.split(' ', expand=True)\n","    df_aux['DRS'] = DRS_expand[1]\n","\n","    # Drop unnecessary columns\n","    cols = df_aux.columns\n","    drop_cols = ['ESCOLARI', 'UFNASC', 'UFRESID', 'CIDADE', 'DTCONSULT', 'CLINICA',\n","                 'DTDIAG', 'BASEDIAG', 'DESCTOPO', 'MORFO', 'DESCMORFO', 'ECGRUP',\n","                 'T', 'N', 'M', 'PT', 'PN', 'PM', 'S', 'G', 'LOCALTNM', 'IDMITOTIC',\n","                 'PSA', 'GLEASON', 'OUTRACLA', 'META01', 'META02', 'META03',\n","                 'META04', 'DTTRAT', 'NAOTRAT', 'TRATAMENTO', 'TRATFANTES',\n","                 'TRATFAPOS', 'NENHUMANT', 'CIRURANT', 'RADIOANT', 'QUIMIOANT',\n","                 'HORMOANT', 'TMOANT', 'IMUNOANT', 'OUTROANT', 'NENHUMAPOS',\n","                 'CIRURAPOS', 'RADIOAPOS', 'QUIMIOAPOS', 'HORMOAPOS', 'TMOAPOS',\n","                 'IMUNOAPOS', 'OUTROAPOS', 'DTULTINFO', 'CICI', 'CICIGRUP',\n","                 'CICISUBGRU', 'FAIXAETAR', 'LATERALI', 'INSTORIG', 'PERDASEG',\n","                 'ERRO', 'DTRECIDIVA', 'RECNENHUM', 'RECLOCAL', 'RECREGIO',\n","                 'RECDIST', 'REC01', 'REC02', 'REC03', 'REC04', 'CIDO', 'DSCCIDO',\n","                 'HABILIT', 'HABIT11', 'HABILIT1', 'CIDADEH']  # List of columns to be dropped\n","    cols = cols.drop(drop_cols)\n","\n","    return df_aux[cols]\n","\n","#######################################################################################\n","\n","def get_labels(df):\n","    \"\"\"\n","    Creates new columns indicating the patient's survival status.\n","\n","    Args:\n","        df (pandas.DataFrame): A DataFrame containing patient data, including\n","            'ULTINFO' (last status indicator) and 'ULTIDIAG' (time in days since diagnosis).\n","\n","    Returns:\n","        pandas.DataFrame: A DataFrame with the following added columns:\n","            - 'obito_geral': Indicates whether the patient died (1) or not (0).\n","            - 'sobrevida_ano1': Indicates if the patient survived at least 1 year after diagnosis.\n","            - 'sobrevida_ano3': Indicates if the patient survived at least 3 years after diagnosis.\n","            - 'sobrevida_ano5': Indicates if the patient survived at least 5 years after diagnosis.\n","    \"\"\"\n","\n","    df_aux = df.copy()\n","\n","    # Initialize survival status columns with 0 (event not occurred)\n","    df_aux['obito_geral'] = 0\n","    df_aux['sobrevida_ano1'] = 0\n","    df_aux['sobrevida_ano3'] = 0\n","    df_aux['sobrevida_ano5'] = 0\n","\n","    # Define overall death based on ULTINFO\n","    df_aux.loc[df_aux.ULTINFO > 2, 'obito_geral'] = 1\n","\n","    # Define survival at 1, 3, and 5 years based on ULTIDIAG (assuming ULTIDIAG is in days)\n","    df_aux.loc[df_aux.ULTIDIAG > 365, 'sobrevida_ano1'] = 1\n","    df_aux.loc[df_aux.ULTIDIAG > 3 * 365, 'sobrevida_ano3'] = 1\n","    df_aux.loc[df_aux.ULTIDIAG > 5 * 365, 'sobrevida_ano5'] = 1\n","\n","    # Drop original columns 'ULTINFO' and 'ULTIDIAG'\n","    drop_cols = ['ULTINFO', 'ULTIDIAG']\n","\n","    return df_aux.drop(columns=drop_cols)\n","\n","#######################################################################################\n","\n","def get_train_test(df, drop_cols, label, test_size=0.25, random_state=0):\n","    \"\"\"\n","    Splits a DataFrame into training and testing sets.\n","\n","    Args:\n","        df (pandas.DataFrame): The DataFrame to be split.\n","        drop_cols (list): A list of column names to be dropped.\n","        label (str): The name of the target variable.\n","        test_size (float, optional): The proportion of the dataset to include in the test split. Defaults to 0.25.\n","        random_state (int, optional): Controls the shuffling applied to the data before applying the split.\n","                                      Pass an int for reproducible output across multiple function calls. Defaults to 0.\n","\n","    Returns:\n","        tuple: A tuple containing four DataFrames:\n","            X_train: Training set features.\n","            X_test: Testing set features.\n","            y_train: Training set labels.\n","            y_test: Testing set labels.\n","    \"\"\"\n","\n","    df_aux = df.copy()\n","\n","    # Select features and target variable\n","    features = df_aux.drop(columns=drop_cols)\n","    target = df_aux[label]\n","\n","    # Split data into training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(features, target,\n","                                                    test_size=test_size,\n","                                                    random_state=random_state,\n","                                                    stratify=target)\n","\n","    return X_train, X_test, y_train, y_test\n","\n","#######################################################################################\n","\n","def train_preprocessing(df, ohe_encoder=None, normalizer='StandardScaler'):\n","    \"\"\"\n","    Performs preprocessing on the training dataset.\n","\n","    Args:\n","        df (pandas.DataFrame): The DataFrame to be preprocessed.\n","        ohe_encoder (list, optional): A list of columns to apply one-hot encoding. If None,\n","            all categorical columns will be encoded using LabelEncoder.\n","        normalizer (str, optional): The type of normalization to apply. Defaults to 'StandardScaler'.\n","            Options include: 'StandardScaler', 'MinMaxScaler', 'MaxAbsScaler', 'QuantileTransformer'.\n","\n","    Returns:\n","        tuple: A tuple containing:\n","            - df_aux (pandas.DataFrame): The preprocessed DataFrame.\n","            - enc (dict): A dictionary of encoders, one for each categorical variable.\n","            - norm: The normalization object used.\n","            - feat_cols (list): A list of feature column names.\n","    \"\"\"\n","\n","    df_aux = df.copy()\n","\n","    enc = {}  # Dictionary to store encoders\n","    if ohe_encoder:\n","        # Apply one-hot encoding to specified columns\n","        for col in ohe_encoder:\n","            enc[col] = OneHotEncoder(handle_unknown='ignore', drop='first')\n","            ohe_results = enc[col].fit_transform(df_aux[[col]])\n","            # ... (rest of the one-hot encoding logic)\n","\n","    # Apply label encoding to remaining categorical columns\n","    list_categorical = df_aux.select_dtypes(include='object').columns\n","    for col in list_categorical:\n","        enc[col] = LabelEncoder()\n","        df_aux[col] = enc[col].fit_transform(df_aux[col])\n","\n","    # Select feature columns\n","    feat_cols = df_aux.columns\n","\n","    # Apply normalization\n","    if normalizer == 'StandardScaler':\n","        norm = StandardScaler()\n","    elif normalizer == 'MinMaxScaler':\n","        norm = MinMaxScaler((0, 1))\n","    elif normalizer == 'MaxAbsScaler':\n","        norm = MaxAbsScaler()\n","    elif normalizer == 'QuantileTransformer':\n","        norm = QuantileTransformer(output_distribution='normal')\n","\n","    df_aux = norm.fit_transform(df_aux)\n","\n","    return df_aux, enc, norm, feat_cols\n","\n","#######################################################################################\n","\n","def test_preprocessing(df, enc, norm, ohe_encoder=None):\n","    \"\"\"\n","    Performs preprocessing on the test dataset using the provided encoders and normalizer.\n","\n","    Args:\n","        df (pandas.DataFrame): The DataFrame to be preprocessed.\n","        enc (dict): A dictionary of encoders used for categorical features in the training set.\n","        norm: The normalization object used for the training set.\n","        ohe_encoder (list, optional): A list of columns to apply one-hot encoding. If None,\n","            all categorical columns will be encoded using LabelEncoder.\n","\n","    Returns:\n","        pandas.DataFrame: The preprocessed DataFrame.\n","    \"\"\"\n","\n","    df_aux = df.copy()\n","\n","    if ohe_encoder:\n","        # Apply one-hot encoding to specified columns using pre-trained encoders\n","        for col in ohe_encoder:\n","            ohe_results = enc[col].transform(df_aux[[col]])\n","            df1 = pd.DataFrame(ohe_results.toarray(),\n","                               columns=enc[col].get_feature_names_out(),\n","                               index=df_aux[col].index)\n","            df_aux = df_aux.merge(df1, how='left', left_index=True, right_index=True)\n","\n","        df_aux.drop(columns=ohe_encoder, inplace=True)\n","\n","    # Apply label encoding to remaining categorical columns\n","    # Handle unseen categories by assigning them a new category (e.g., -1)\n","    list_categorical = df_aux.select_dtypes(include='object').columns\n","    for col in list_categorical:\n","        df_aux.loc[~df_aux[col].isin(enc[col].classes_), col] = -1\n","        df_aux.loc[df_aux[col].isin(enc[col].classes_), col] = enc[col].transform(df_aux[col][df_aux[col].isin(enc[col].classes_)])\n","\n","    # Apply normalization using the pre-trained scaler\n","    df_aux = norm.transform(df_aux)\n","\n","    return df_aux\n","\n","#######################################################################################\n","\n","def preprocessing(df, cols_drop, label, test_size=0.25, ohe_encoder=None,\n","                  norm_name='StandardScaler', return_enc_norm=False,\n","                  balance_data=False, random_state=0):\n","    \"\"\"\n","    Performs comprehensive data preprocessing, including train-test split,\n","    categorical encoding, normalization, and optional class balancing.\n","\n","    Args:\n","        df: The DataFrame containing the raw data.\n","        cols_drop: A list of columns to be dropped.\n","        label: The name of the target variable.\n","        test_size: The proportion of the dataset to include in the test split.\n","        ohe_encoder: A list of columns for one-hot encoding.\n","        norm_name: The name of the normalizer to use.\n","        return_enc_norm: Whether to return the encoders and normalizer.\n","        balance_data: Whether to balance the dataset using SMOTE.\n","        random_state: Random seed for reproducibility.\n","\n","    Returns:\n","        A tuple containing:\n","            - X_train_: Preprocessed training features.\n","            - X_test_: Preprocessed testing features.\n","            - y_train_: Training labels.\n","            - y_test_: Testing labels.\n","            - feat_cols: Feature column names after preprocessing.\n","            - enc (optional): Dictionary of encoders used.\n","            - norm (optional): Normalization object used.\n","    \"\"\"\n","\n","    # Split data into training and testing sets\n","    X_train, X_test, y_train, y_test = get_train_test(df_aux, cols_drop, label,\n","                                                      test_size,\n","                                                      random_state=random_state)\n","\n","    # Preprocess training set\n","    X_train_enc, enc, norm, feat_cols = train_preprocessing(X_train, ohe_encoder=ohe_encoder,\n","                                                            normalizer=norm_name)\n","    # Preprocess testing set using the same encoders and normalizer\n","    X_test_ = test_preprocessing(X_test, enc, norm, ohe_encoder=ohe_encoder)\n","\n","    # Balance training data if specified\n","    if balance_data:\n","        X_train_, y_train_ = SMOTE(random_state=random_state).fit_resample(X_train_enc, y_train)\n","\n","    # Print shapes for verification\n","    print(f'X_train = {X_train_.shape}, X_test = {X_test_.shape}')\n","    print(f'y_train = {y_train_.shape}, y_test = {y_test.shape}')\n","\n","    # Return results\n","    if return_enc_norm:\n","        return X_train_, X_test_, y_train_, y_test, feat_cols, enc, norm\n","    else:\n","        return X_train_, X_test_, y_train_, y_test, feat_cols\n","\n","#######################################################################################\n","\n","def show_tree(model, feat_cols, max_depth=3, estimator=0):\n","    \"\"\"\n","    Plots a decision tree from a RandomForestClassifier.\n","\n","    Args:\n","        model (RandomForestClassifier): The trained RandomForestClassifier model.\n","        feat_cols (array-like): A list or array of feature names used in training.\n","        max_depth (int, optional): The maximum depth of the tree to display. Defaults to 3.\n","        estimator (int, optional): The index of the estimator to display. Defaults to 0.\n","\n","    Returns:\n","        None: Displays the plotted tree.\n","    \"\"\"\n","\n","    # Create a figure for the plot\n","    plt.figure(figsize=(22, 10))\n","\n","    # Plot the specified tree from the Random Forest\n","    tree.plot_tree(model.estimators_[estimator],\n","                  feature_names=feat_cols,\n","                  filled=True,  # Color leaf nodes\n","                  max_depth=max_depth)\n","    plt.show()  # Display the plot\n","\n","#######################################################################################\n","\n","def plot_feat_importances(model, feat_cols, n=10):\n","    \"\"\"\n","    Plots a horizontal bar chart of the top n most important features in a model.\n","\n","    Args:\n","        model: The trained machine learning model.\n","        feat_cols: A list or array of feature names.\n","        n: The number of top features to plot. Defaults to 10.\n","\n","    Returns:\n","        None: Displays the plot.\n","    \"\"\"\n","\n","    # Get feature importances from the model\n","    feat_import = pd.Series(model.feature_importances_, index=feat_cols)\n","\n","    # Plot the top n features\n","    feat_import.nlargest(n).plot(kind='barh', figsize=(10, 8))\n","    plt.show()\n","\n","#######################################################################################\n","\n","def plot_roc_curve(model, X_train, X_test, y_train, y_test):\n","    \"\"\"\n","    Plots the Receiver Operating Characteristic (ROC) curve for a classification model,\n","    calculating the Area Under the Curve (AUC) for both training and testing sets.\n","\n","    Args:\n","        model: A trained classification model with a `predict_proba` method.\n","        X_train: Training features.\n","        X_test: Testing features.\n","        y_train: Training labels.\n","        y_test: Testing labels.\n","\n","    Returns:\n","        None: Displays the ROC curve plot.\n","    \"\"\"\n","\n","    # Get predicted probabilities for positive class\n","    probas_train = model.predict_proba(X_train)[:, 1]\n","    probas_test = model.predict_proba(X_test)[:, 1]\n","\n","    # Calculate true positive rate (TPR) and false positive rate (FPR)\n","    fp_train, tp_train, _ = roc_curve(y_train, probas_train)\n","    fp_test, tp_test, _ = roc_curve(y_test, probas_test)\n","\n","    # Calculate AUC\n","    auc_train = auc(fp_train, tp_train)\n","    auc_test = auc(fp_test, tp_test)\n","\n","    # Plot ROC curves\n","    plt.figure(figsize=(10, 7))\n","    plt.plot(fp_train, tp_train, 'b', label=f'Train (AUC = {auc_train:.4f})')\n","    plt.plot(fp_test, tp_test, 'r', label=f'Test (AUC = {auc_test:.4f})')\n","\n","    # Plot random guessing line\n","    plt.plot(np.linspace(0, 1, 100),\n","             np.linspace(0, 1, 100),\n","             label='Baseline',\n","             linestyle='--',\n","             color='k')\n","\n","    # Set plot labels and display\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.grid(True)\n","    plt.legend()\n","    plt.show()\n","\n","#######################################################################################\n","\n","def plot_confusion_matrix(model, x, y, format='.5f', norm=True):\n","    \"\"\"\n","    Plots a normalized confusion matrix and classification report for a given model.\n","\n","    Args:\n","        model: A trained classification model.\n","        x: The features of the test set.\n","        y: The true labels of the test set.\n","        format (str, optional): Format specifier for the values in the confusion matrix.\n","            Defaults to '.5f' (5 decimal places).\n","        norm (bool, optional): If True, normalizes the confusion matrix. Defaults to True.\n","\n","    Returns:\n","        None: Displays the confusion matrix and classification report.\n","    \"\"\"\n","\n","    # Create a confusion matrix plot\n","    with plt.rc_context({'font.size': 12, 'font.weight': 'bold'}):\n","        if norm:\n","            # Plot a normalized confusion matrix\n","            ConfusionMatrixDisplay.from_estimator(model, x, y, values_format=format,\n","                                                 cmap='binary', normalize='true')\n","        else:\n","            # Plot a non-normalized confusion matrix\n","            ConfusionMatrixDisplay.from_estimator(model, x, y, cmap='binary',\n","                                                 values_format='.0f')\n","        plt.show()\n","\n","    # Generate a classification report\n","    pred = model.predict(x)\n","    print(f'\\n{classification_report(y, pred, digits=5)}')\n","\n","#######################################################################################\n","\n","def plot_shap_values(model, x, features, max_display=10):\n","    \"\"\"\n","    Plots a summary plot of SHAP values to visualize feature importance.\n","\n","    Args:\n","        model: A trained machine learning model.\n","        x: A pandas DataFrame containing the features used to train the model.\n","        features: A list of feature names.\n","        max_display: The maximum number of features to display in the plot.\n","\n","    Returns:\n","        None: Displays the SHAP summary plot.\n","    \"\"\"\n","\n","    # Calculate SHAP values\n","    explainer = shap.TreeExplainer(model)\n","    shap_values = explainer.shap_values(x)\n","\n","    # Plot summary plot\n","    try:\n","        # For models with multiple outputs\n","        shap.summary_plot(shap_values[1], x, feature_names=features, max_display=max_display)\n","    except AssertionError:\n","        # For models with a single output\n","        shap.summary_plot(shap_values, x, feature_names=features, max_display=max_display)\n","\n","#######################################################################################\n","\n","def pred_cruzada(df, model, list_drop, label, enc, norm):\n","    \"\"\"\n","    Performs cross-validation and plots the confusion matrix.\n","\n","    Args:\n","        df: The DataFrame containing the data.\n","        model: A trained classification model.\n","        list_drop: A list of columns to drop.\n","        label: The name of the target variable.\n","        enc: A dictionary of encoders used for preprocessing.\n","        norm: The normalization object used.\n","\n","    Returns:\n","        None: Displays the confusion matrix.\n","    \"\"\"\n","\n","    # Filter data for survival analysis\n","    df_sobrevida = df[~((df.obito_geral == 0) & (df[label] == 0))]\n","    df_sobrevida.reset_index(drop=True, inplace=True)\n","\n","    # Extract target variable\n","    y = df_sobrevida[label].values\n","\n","    # Extract features and preprocess\n","    X = df_sobrevida.drop(columns=list_drop)\n","    X_pp = test_preprocessing(X, enc, norm)\n","\n","    # Plot confusion matrix\n","    plot_confusion_matrix(model, X_pp, y)"],"metadata":{"id":"qEr1sobD1WOW"},"execution_count":null,"outputs":[]}]}